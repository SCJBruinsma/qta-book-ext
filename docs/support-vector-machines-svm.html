<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.1 Support Vector Machines (SVM) | Introduction to Quantitative Text Analysis</title>
  <meta name="description" content="7.1 Support Vector Machines (SVM) | Introduction to Quantitative Text Analysis" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="7.1 Support Vector Machines (SVM) | Introduction to Quantitative Text Analysis" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.1 Support Vector Machines (SVM) | Introduction to Quantitative Text Analysis" />
  
  
  

<meta name="author" content="Kostas Gemenis and Bastiaan Bruinsma" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="supervised-methods.html"/>
<link rel="next" href="logistic-regression.html"/>
<script src="assets/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="assets/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="assets/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="assets/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 2em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Quantitative Text Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="1" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="installing-r.html"><a href="installing-r.html"><i class="fa fa-check"></i><b>1.1</b> Installing R</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="installing-r.html"><a href="installing-r.html#windows"><i class="fa fa-check"></i><b>1.1.1</b> Windows</a></li>
<li class="chapter" data-level="1.1.2" data-path="installing-r.html"><a href="installing-r.html#linux"><i class="fa fa-check"></i><b>1.1.2</b> Linux</a></li>
<li class="chapter" data-level="1.1.3" data-path="installing-r.html"><a href="installing-r.html#macos"><i class="fa fa-check"></i><b>1.1.3</b> macOS</a></li>
<li class="chapter" data-level="1.1.4" data-path="installing-r.html"><a href="installing-r.html#cloud"><i class="fa fa-check"></i><b>1.1.4</b> Cloud</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="installing-packages.html"><a href="installing-packages.html"><i class="fa fa-check"></i><b>1.2</b> Installing Packages</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="installing-packages.html"><a href="installing-packages.html#cran"><i class="fa fa-check"></i><b>1.2.1</b> CRAN</a></li>
<li class="chapter" data-level="1.2.2" data-path="installing-packages.html"><a href="installing-packages.html#github"><i class="fa fa-check"></i><b>1.2.2</b> GitHub</a></li>
<li class="chapter" data-level="1.2.3" data-path="installing-packages.html"><a href="installing-packages.html#writing-packages"><i class="fa fa-check"></i><b>1.2.3</b> Writing Packages</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="required.html"><a href="required.html"><i class="fa fa-check"></i><b>1.3</b> Required Packages</a></li>
<li class="chapter" data-level="1.4" data-path="troubleshooting.html"><a href="troubleshooting.html"><i class="fa fa-check"></i><b>1.4</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>2</b> Background</a>
<ul>
<li class="chapter" data-level="2.1" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>2.1</b> Concepts</a></li>
<li class="chapter" data-level="2.2" data-path="workflow.html"><a href="workflow.html"><i class="fa fa-check"></i><b>2.2</b> Workflow</a></li>
<li class="chapter" data-level="2.3" data-path="validation.html"><a href="validation.html"><i class="fa fa-check"></i><b>2.3</b> Validation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="validation.html"><a href="validation.html#validity"><i class="fa fa-check"></i><b>2.3.1</b> Validity</a></li>
<li class="chapter" data-level="2.3.2" data-path="validation.html"><a href="validation.html#reliability"><i class="fa fa-check"></i><b>2.3.2</b> Reliability</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="import.html"><a href="import.html"><i class="fa fa-check"></i><b>3</b> Text in R</a>
<ul>
<li class="chapter" data-level="3.1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>3.1</b> Basics</a></li>
<li class="chapter" data-level="3.2" data-path="import-.html"><a href="import-.html"><i class="fa fa-check"></i><b>3.2</b> Import .txt</a></li>
<li class="chapter" data-level="3.3" data-path="import-.html"><a href="import-.html#import-.pdf"><i class="fa fa-check"></i><b>3.3</b> Import .pdf</a></li>
<li class="chapter" data-level="3.4" data-path="import-.html"><a href="import-.html#import-.csv"><i class="fa fa-check"></i><b>3.4</b> Import .csv</a></li>
<li class="chapter" data-level="3.5" data-path="import-from-an-api.html"><a href="import-from-an-api.html"><i class="fa fa-check"></i><b>3.5</b> Import from an API</a></li>
<li class="chapter" data-level="3.6" data-path="import-using-web-scraping.html"><a href="import-using-web-scraping.html"><i class="fa fa-check"></i><b>3.6</b> Import using Web Scraping</a></li>
<li class="chapter" data-level="3.7" data-path="import-json-and-xml.html"><a href="import-json-and-xml.html"><i class="fa fa-check"></i><b>3.7</b> Import JSON and XML</a></li>
<li class="chapter" data-level="3.8" data-path="import-from-databases.html"><a href="import-from-databases.html"><i class="fa fa-check"></i><b>3.8</b> Import from Databases</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="describe.html"><a href="describe.html"><i class="fa fa-check"></i><b>4</b> Describe</a>
<ul>
<li class="chapter" data-level="4.1" data-path="corpus-and-dfm.html"><a href="corpus-and-dfm.html"><i class="fa fa-check"></i><b>4.1</b> Corpus and DFM</a></li>
<li class="chapter" data-level="4.2" data-path="text-pre-processing.html"><a href="text-pre-processing.html"><i class="fa fa-check"></i><b>4.2</b> Text Pre-processing</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="text-pre-processing.html"><a href="text-pre-processing.html#tokenisation-and-initial-cleaning"><i class="fa fa-check"></i><b>4.2.1</b> Tokenisation and Initial Cleaning</a></li>
<li class="chapter" data-level="4.2.2" data-path="text-pre-processing.html"><a href="text-pre-processing.html#lower-casing"><i class="fa fa-check"></i><b>4.2.2</b> Lower-casing</a></li>
<li class="chapter" data-level="4.2.3" data-path="text-pre-processing.html"><a href="text-pre-processing.html#stopword-removal"><i class="fa fa-check"></i><b>4.2.3</b> Stopword Removal</a></li>
<li class="chapter" data-level="4.2.4" data-path="text-pre-processing.html"><a href="text-pre-processing.html#n-grams-and-collocations"><i class="fa fa-check"></i><b>4.2.4</b> N-grams and Collocations</a></li>
<li class="chapter" data-level="4.2.5" data-path="text-pre-processing.html"><a href="text-pre-processing.html#stemming-and-lemmatisation"><i class="fa fa-check"></i><b>4.2.5</b> Stemming and Lemmatisation</a></li>
<li class="chapter" data-level="4.2.6" data-path="text-pre-processing.html"><a href="text-pre-processing.html#removing-sparse-features-dfm-trimming"><i class="fa fa-check"></i><b>4.2.6</b> Removing Sparse Features (DFM Trimming)</a></li>
<li class="chapter" data-level="4.2.7" data-path="text-pre-processing.html"><a href="text-pre-processing.html#additional-pre-processing"><i class="fa fa-check"></i><b>4.2.7</b> Additional Pre-Processing</a></li>
<li class="chapter" data-level="4.2.8" data-path="text-pre-processing.html"><a href="text-pre-processing.html#evaluating-pre-processing"><i class="fa fa-check"></i><b>4.2.8</b> Evaluating Pre-Processing</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="descriptives-and-visualisations.html"><a href="descriptives-and-visualisations.html"><i class="fa fa-check"></i><b>4.3</b> Descriptives and Visualisations</a></li>
<li class="chapter" data-level="4.4" data-path="text-statistics.html"><a href="text-statistics.html"><i class="fa fa-check"></i><b>4.4</b> Text Statistics</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="text-statistics.html"><a href="text-statistics.html#summary"><i class="fa fa-check"></i><b>4.4.1</b> Summary</a></li>
<li class="chapter" data-level="4.4.2" data-path="text-statistics.html"><a href="text-statistics.html#frequencies"><i class="fa fa-check"></i><b>4.4.2</b> Frequencies</a></li>
<li class="chapter" data-level="4.4.3" data-path="text-statistics.html"><a href="text-statistics.html#lexical-diversity"><i class="fa fa-check"></i><b>4.4.3</b> Lexical diversity</a></li>
<li class="chapter" data-level="4.4.4" data-path="text-statistics.html"><a href="text-statistics.html#readability"><i class="fa fa-check"></i><b>4.4.4</b> Readability</a></li>
<li class="chapter" data-level="4.4.5" data-path="text-statistics.html"><a href="text-statistics.html#similarity-and-distance"><i class="fa fa-check"></i><b>4.4.5</b> Similarity and Distance</a></li>
<li class="chapter" data-level="4.4.6" data-path="text-statistics.html"><a href="text-statistics.html#keyness"><i class="fa fa-check"></i><b>4.4.6</b> Keyness</a></li>
<li class="chapter" data-level="4.4.7" data-path="text-statistics.html"><a href="text-statistics.html#entropy"><i class="fa fa-check"></i><b>4.4.7</b> Entropy</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="dictionary-analysis.html"><a href="dictionary-analysis.html"><i class="fa fa-check"></i><b>5</b> Dictionary Analysis</a>
<ul>
<li class="chapter" data-level="5.1" data-path="classical-dictionary-analysis.html"><a href="classical-dictionary-analysis.html"><i class="fa fa-check"></i><b>5.1</b> Classical Dictionary Analysis</a></li>
<li class="chapter" data-level="5.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html"><i class="fa fa-check"></i><b>5.2</b> Sentiment Analysis</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#movie-reviews"><i class="fa fa-check"></i><b>5.2.1</b> Movie Reviews</a></li>
<li class="chapter" data-level="5.2.2" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#twitter"><i class="fa fa-check"></i><b>5.2.2</b> Twitter</a></li>
<li class="chapter" data-level="5.2.3" data-path="sentiment-analysis.html"><a href="sentiment-analysis.html#vader"><i class="fa fa-check"></i><b>5.2.3</b> VADER</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>5.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="scaling.html"><a href="scaling.html"><i class="fa fa-check"></i><b>6</b> Scaling Methods</a>
<ul>
<li class="chapter" data-level="6.1" data-path="wordscores.html"><a href="wordscores.html"><i class="fa fa-check"></i><b>6.1</b> Wordscores</a></li>
<li class="chapter" data-level="6.2" data-path="wordfish.html"><a href="wordfish.html"><i class="fa fa-check"></i><b>6.2</b> Wordfish</a></li>
<li class="chapter" data-level="6.3" data-path="correspondence-analysis.html"><a href="correspondence-analysis.html"><i class="fa fa-check"></i><b>6.3</b> Correspondence Analysis</a></li>
<li class="chapter" data-level="6.4" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="supervised-methods.html"><a href="supervised-methods.html"><i class="fa fa-check"></i><b>7</b> Supervised Methods</a>
<ul>
<li class="chapter" data-level="7.1" data-path="support-vector-machines-svm.html"><a href="support-vector-machines-svm.html"><i class="fa fa-check"></i><b>7.1</b> Support Vector Machines (SVM)</a></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="7.3" data-path="naive-bayes-nb.html"><a href="naive-bayes-nb.html"><i class="fa fa-check"></i><b>7.3</b> Naive Bayes (NB)</a></li>
<li class="chapter" data-level="7.4" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="unsupervised-methods.html"><a href="unsupervised-methods.html"><i class="fa fa-check"></i><b>8</b> Unsupervised Methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="latent-dirichlet-allocation-lda.html"><a href="latent-dirichlet-allocation-lda.html"><i class="fa fa-check"></i><b>8.1</b> Latent Dirichlet Allocation (LDA)</a></li>
<li class="chapter" data-level="8.2" data-path="seeded-latent-dirichlet-allocation-slda.html"><a href="seeded-latent-dirichlet-allocation-slda.html"><i class="fa fa-check"></i><b>8.2</b> Seeded Latent Dirichlet Allocation (sLDA)</a></li>
<li class="chapter" data-level="8.3" data-path="structural-topic-model-stm.html"><a href="structural-topic-model-stm.html"><i class="fa fa-check"></i><b>8.3</b> Structural Topic Model (STM)</a></li>
<li class="chapter" data-level="8.4" data-path="latent-semantic-analysis-lsa.html"><a href="latent-semantic-analysis-lsa.html"><i class="fa fa-check"></i><b>8.4</b> Latent Semantic Analysis (LSA)</a></li>
<li class="chapter" data-level="8.5" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>8.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="bastiaan.bruinsma@gmail.com" target="blank">Bastiaan Bruinsma</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Quantitative Text Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-machines-svm" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Support Vector Machines (SVM)<a href="support-vector-machines-svm.html#support-vector-machines-svm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SVMs are powerful supervised learning models that are frequently employed for classification tasks. In text classification, for example, they operate by mapping features of documents, such as word counts, from a document-feature matrix (DFM) into a high-dimensional space. In this space, SVMs identify the optimal hyperplane that most effectively separates documents into different categories.</p>
<p>Imagine you have a piece of paper with red and blue dots scattered on it, and you want to draw a straight line that best separates them. An SVM tries to do something similar but in many more dimensions. Each document can be considered a point in a very high-dimensional space. Each dimension corresponds to a unique word in your vocabulary, and the position of the document along that dimension depends on how frequently that word appears in the document or on other measures, such as TF-IDF. An SVM aims to find the optimal hyperplane that best separates documents into different categories, such as “positive review” versus “negative review”. ‘Best’ means the hyperplane with the largest possible margin — the broadest possible gap — between the closest documents of the different classes. Documents closest to this boundary, which defines the margin, are called ‘support vectors’. SVMs are known for their effectiveness, especially when dealing with many features, which is common in text data since each word can constitute a feature.</p>
<p>In this example, we will use the <code>caret</code> package to train an SVM model for binary polarity classification, use <code>quanteda</code> for text preprocessing and the binary polarity label (“neg” or “pos”) provided by a sample of the Large Movie Review Dataset (<code>data_corpus_LMRD</code>) as the target variable:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="support-vector-machines-svm.html#cb203-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb203-2"><a href="support-vector-machines-svm.html#cb203-2" tabindex="-1"></a></span>
<span id="cb203-3"><a href="support-vector-machines-svm.html#cb203-3" tabindex="-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb203-4"><a href="support-vector-machines-svm.html#cb203-4" tabindex="-1"></a><span class="fu">library</span>(quanteda.classifiers)</span>
<span id="cb203-5"><a href="support-vector-machines-svm.html#cb203-5" tabindex="-1"></a><span class="fu">library</span>(caret)  <span class="co"># For model training, evaluation, and cross-validation</span></span>
<span id="cb203-6"><a href="support-vector-machines-svm.html#cb203-6" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb203-7"><a href="support-vector-machines-svm.html#cb203-7" tabindex="-1"></a><span class="fu">library</span>(pROC)  <span class="co"># For ROC analysis</span></span>
<span id="cb203-8"><a href="support-vector-machines-svm.html#cb203-8" tabindex="-1"></a></span>
<span id="cb203-9"><a href="support-vector-machines-svm.html#cb203-9" tabindex="-1"></a>corpus_reviews <span class="ot">&lt;-</span> <span class="fu">corpus_sample</span>(data_corpus_LMRD, <span class="dv">2000</span>)  <span class="co"># Sample 2000 reviews</span></span></code></pre></div>
<p>We need to split our data into separate training and test sets for supervised learning: we will use the training set to build the model and the test set to evaluate its performance on unseen data later on. We will use the <code>polarity</code> variable as our target, ensuring it is a factor as this is the required format for classification tasks in <code>caret</code>. Note that we will filter for documents with non-missing polarity labels before splitting. <code>caret</code> requires factor levels to be valid R variable names (like “neg” and “pos”):</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="support-vector-machines-svm.html#cb204-1" tabindex="-1"></a>polarity_labels <span class="ot">&lt;-</span> <span class="fu">factor</span>(corpus_reviews<span class="sc">$</span>polarity)</span>
<span id="cb204-2"><a href="support-vector-machines-svm.html#cb204-2" tabindex="-1"></a></span>
<span id="cb204-3"><a href="support-vector-machines-svm.html#cb204-3" tabindex="-1"></a><span class="co"># Identify documents with valid polarity labels</span></span>
<span id="cb204-4"><a href="support-vector-machines-svm.html#cb204-4" tabindex="-1"></a></span>
<span id="cb204-5"><a href="support-vector-machines-svm.html#cb204-5" tabindex="-1"></a>valid_docs_index <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="sc">!</span><span class="fu">is.na</span>(polarity_labels))</span>
<span id="cb204-6"><a href="support-vector-machines-svm.html#cb204-6" tabindex="-1"></a>corpus_reviews_valid <span class="ot">&lt;-</span> corpus_reviews[valid_docs_index]</span>
<span id="cb204-7"><a href="support-vector-machines-svm.html#cb204-7" tabindex="-1"></a>polarity_valid <span class="ot">&lt;-</span> polarity_labels[valid_docs_index]</span>
<span id="cb204-8"><a href="support-vector-machines-svm.html#cb204-8" tabindex="-1"></a>polarity_valid <span class="ot">&lt;-</span> <span class="fu">factor</span>(polarity_valid, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;neg&quot;</span>, <span class="st">&quot;pos&quot;</span>))</span>
<span id="cb204-9"><a href="support-vector-machines-svm.html#cb204-9" tabindex="-1"></a></span>
<span id="cb204-10"><a href="support-vector-machines-svm.html#cb204-10" tabindex="-1"></a><span class="co"># Check the distribution of polarity_valid BEFORE splitting</span></span>
<span id="cb204-11"><a href="support-vector-machines-svm.html#cb204-11" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">table</span>(polarity_valid))</span></code></pre></div>
<pre><code>## polarity_valid
##  neg  pos 
## 1023  977</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="support-vector-machines-svm.html#cb206-1" tabindex="-1"></a><span class="co"># Create stratified split indices to ensure both classes are included in</span></span>
<span id="cb206-2"><a href="support-vector-machines-svm.html#cb206-2" tabindex="-1"></a><span class="co"># train/test sets</span></span>
<span id="cb206-3"><a href="support-vector-machines-svm.html#cb206-3" tabindex="-1"></a>neg_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(polarity_valid <span class="sc">==</span> <span class="st">&quot;neg&quot;</span>)</span>
<span id="cb206-4"><a href="support-vector-machines-svm.html#cb206-4" tabindex="-1"></a>pos_indices <span class="ot">&lt;-</span> <span class="fu">which</span>(polarity_valid <span class="sc">==</span> <span class="st">&quot;pos&quot;</span>)</span>
<span id="cb206-5"><a href="support-vector-machines-svm.html#cb206-5" tabindex="-1"></a></span>
<span id="cb206-6"><a href="support-vector-machines-svm.html#cb206-6" tabindex="-1"></a><span class="co"># Determine the number of instances for train/test per class (70/30 split)</span></span>
<span id="cb206-7"><a href="support-vector-machines-svm.html#cb206-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb206-8"><a href="support-vector-machines-svm.html#cb206-8" tabindex="-1"></a>train_size_neg <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.7</span> <span class="sc">*</span> <span class="fu">length</span>(neg_indices))</span>
<span id="cb206-9"><a href="support-vector-machines-svm.html#cb206-9" tabindex="-1"></a>train_size_pos <span class="ot">&lt;-</span> <span class="fu">floor</span>(<span class="fl">0.7</span> <span class="sc">*</span> <span class="fu">length</span>(pos_indices))</span>
<span id="cb206-10"><a href="support-vector-machines-svm.html#cb206-10" tabindex="-1"></a></span>
<span id="cb206-11"><a href="support-vector-machines-svm.html#cb206-11" tabindex="-1"></a><span class="co"># Sample indices for training set from each class</span></span>
<span id="cb206-12"><a href="support-vector-machines-svm.html#cb206-12" tabindex="-1"></a>train_indices_neg <span class="ot">&lt;-</span> <span class="fu">sample</span>(neg_indices, <span class="at">size =</span> train_size_neg, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb206-13"><a href="support-vector-machines-svm.html#cb206-13" tabindex="-1"></a>train_indices_pos <span class="ot">&lt;-</span> <span class="fu">sample</span>(pos_indices, <span class="at">size =</span> train_size_pos, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb206-14"><a href="support-vector-machines-svm.html#cb206-14" tabindex="-1"></a></span>
<span id="cb206-15"><a href="support-vector-machines-svm.html#cb206-15" tabindex="-1"></a><span class="co"># Combine training indices</span></span>
<span id="cb206-16"><a href="support-vector-machines-svm.html#cb206-16" tabindex="-1"></a>train_index <span class="ot">&lt;-</span> <span class="fu">c</span>(train_indices_neg, train_indices_pos)</span>
<span id="cb206-17"><a href="support-vector-machines-svm.html#cb206-17" tabindex="-1"></a></span>
<span id="cb206-18"><a href="support-vector-machines-svm.html#cb206-18" tabindex="-1"></a><span class="co"># The remaining indices are for the test set</span></span>
<span id="cb206-19"><a href="support-vector-machines-svm.html#cb206-19" tabindex="-1"></a>all_valid_indices <span class="ot">&lt;-</span> <span class="fu">seq_along</span>(polarity_valid)</span>
<span id="cb206-20"><a href="support-vector-machines-svm.html#cb206-20" tabindex="-1"></a>test_index <span class="ot">&lt;-</span> all_valid_indices[<span class="sc">!</span>all_valid_indices <span class="sc">%in%</span> train_index]</span>
<span id="cb206-21"><a href="support-vector-machines-svm.html#cb206-21" tabindex="-1"></a></span>
<span id="cb206-22"><a href="support-vector-machines-svm.html#cb206-22" tabindex="-1"></a><span class="co"># Split the corpus subset and polarity labels into training and testing sets</span></span>
<span id="cb206-23"><a href="support-vector-machines-svm.html#cb206-23" tabindex="-1"></a><span class="co"># using the determined indices</span></span>
<span id="cb206-24"><a href="support-vector-machines-svm.html#cb206-24" tabindex="-1"></a>corpus_reviews_train <span class="ot">&lt;-</span> corpus_reviews_valid[train_index]</span>
<span id="cb206-25"><a href="support-vector-machines-svm.html#cb206-25" tabindex="-1"></a>corpus_reviews_test <span class="ot">&lt;-</span> corpus_reviews_valid[test_index]</span>
<span id="cb206-26"><a href="support-vector-machines-svm.html#cb206-26" tabindex="-1"></a></span>
<span id="cb206-27"><a href="support-vector-machines-svm.html#cb206-27" tabindex="-1"></a>polarity_train <span class="ot">&lt;-</span> polarity_valid[train_index]</span>
<span id="cb206-28"><a href="support-vector-machines-svm.html#cb206-28" tabindex="-1"></a>polarity_test <span class="ot">&lt;-</span> polarity_valid[test_index]</span>
<span id="cb206-29"><a href="support-vector-machines-svm.html#cb206-29" tabindex="-1"></a></span>
<span id="cb206-30"><a href="support-vector-machines-svm.html#cb206-30" tabindex="-1"></a><span class="co"># Check the distribution of the split</span></span>
<span id="cb206-31"><a href="support-vector-machines-svm.html#cb206-31" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Training set class distribution:&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Training set class distribution:&quot;</code></pre>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="support-vector-machines-svm.html#cb208-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">table</span>(polarity_train))</span></code></pre></div>
<pre><code>## polarity_train
## neg pos 
## 716 683</code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="support-vector-machines-svm.html#cb210-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">&quot;Testing set class distribution:&quot;</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Testing set class distribution:&quot;</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="support-vector-machines-svm.html#cb212-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">table</span>(polarity_test))</span></code></pre></div>
<pre><code>## polarity_test
## neg pos 
## 307 294</code></pre>
<p>Next, we preprocess the training and test corpus subsets to generate the DFMs. Here, it is crucial that the test DFM has the same features and order as the training DFM. The <code>dfm_match()</code> function ensures this. We will then apply common text cleaning steps during tokenisation and convert the DFMs to matrices to ensure compatibility with <code>caret</code>.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="support-vector-machines-svm.html#cb214-1" tabindex="-1"></a><span class="co"># Tokenise and preprocess the training corpus</span></span>
<span id="cb214-2"><a href="support-vector-machines-svm.html#cb214-2" tabindex="-1"></a></span>
<span id="cb214-3"><a href="support-vector-machines-svm.html#cb214-3" tabindex="-1"></a>tokens_train <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_reviews_train, <span class="at">what =</span> <span class="st">&quot;word&quot;</span>, <span class="at">remove_punct =</span> <span class="cn">TRUE</span>,</span>
<span id="cb214-4"><a href="support-vector-machines-svm.html#cb214-4" tabindex="-1"></a>    <span class="at">remove_symbols =</span> <span class="cn">TRUE</span>, <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>, <span class="at">remove_url =</span> <span class="cn">TRUE</span>, <span class="at">remove_separators =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb214-5"><a href="support-vector-machines-svm.html#cb214-5" tabindex="-1"></a>    <span class="fu">tokens_tolower</span>() <span class="sc">%&gt;%</span></span>
<span id="cb214-6"><a href="support-vector-machines-svm.html#cb214-6" tabindex="-1"></a>    <span class="fu">tokens_select</span>(<span class="fu">stopwords</span>(<span class="st">&quot;english&quot;</span>), <span class="at">selection =</span> <span class="st">&quot;remove&quot;</span>)</span>
<span id="cb214-7"><a href="support-vector-machines-svm.html#cb214-7" tabindex="-1"></a></span>
<span id="cb214-8"><a href="support-vector-machines-svm.html#cb214-8" tabindex="-1"></a><span class="co"># Tokenise and preprocess the test corpus</span></span>
<span id="cb214-9"><a href="support-vector-machines-svm.html#cb214-9" tabindex="-1"></a></span>
<span id="cb214-10"><a href="support-vector-machines-svm.html#cb214-10" tabindex="-1"></a>tokens_test <span class="ot">&lt;-</span> <span class="fu">tokens</span>(corpus_reviews_test, <span class="at">what =</span> <span class="st">&quot;word&quot;</span>, <span class="at">remove_punct =</span> <span class="cn">TRUE</span>, <span class="at">remove_symbols =</span> <span class="cn">TRUE</span>,</span>
<span id="cb214-11"><a href="support-vector-machines-svm.html#cb214-11" tabindex="-1"></a>    <span class="at">remove_numbers =</span> <span class="cn">TRUE</span>, <span class="at">remove_url =</span> <span class="cn">TRUE</span>, <span class="at">remove_separators =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb214-12"><a href="support-vector-machines-svm.html#cb214-12" tabindex="-1"></a>    <span class="fu">tokens_tolower</span>() <span class="sc">%&gt;%</span></span>
<span id="cb214-13"><a href="support-vector-machines-svm.html#cb214-13" tabindex="-1"></a>    <span class="fu">tokens_select</span>(<span class="fu">stopwords</span>(<span class="st">&quot;english&quot;</span>), <span class="at">selection =</span> <span class="st">&quot;remove&quot;</span>)</span>
<span id="cb214-14"><a href="support-vector-machines-svm.html#cb214-14" tabindex="-1"></a></span>
<span id="cb214-15"><a href="support-vector-machines-svm.html#cb214-15" tabindex="-1"></a><span class="co"># Create DFMs from the processed tokens</span></span>
<span id="cb214-16"><a href="support-vector-machines-svm.html#cb214-16" tabindex="-1"></a></span>
<span id="cb214-17"><a href="support-vector-machines-svm.html#cb214-17" tabindex="-1"></a>dfm_train <span class="ot">&lt;-</span> <span class="fu">dfm</span>(tokens_train)</span>
<span id="cb214-18"><a href="support-vector-machines-svm.html#cb214-18" tabindex="-1"></a>dfm_test <span class="ot">&lt;-</span> <span class="fu">dfm</span>(tokens_test)</span>
<span id="cb214-19"><a href="support-vector-machines-svm.html#cb214-19" tabindex="-1"></a></span>
<span id="cb214-20"><a href="support-vector-machines-svm.html#cb214-20" tabindex="-1"></a><span class="co"># Ensure the test DFM has the same features as the training DFM</span></span>
<span id="cb214-21"><a href="support-vector-machines-svm.html#cb214-21" tabindex="-1"></a></span>
<span id="cb214-22"><a href="support-vector-machines-svm.html#cb214-22" tabindex="-1"></a>dfm_test_matched <span class="ot">&lt;-</span> <span class="fu">dfm_match</span>(dfm_test, <span class="at">features =</span> <span class="fu">featnames</span>(dfm_train))</span>
<span id="cb214-23"><a href="support-vector-machines-svm.html#cb214-23" tabindex="-1"></a></span>
<span id="cb214-24"><a href="support-vector-machines-svm.html#cb214-24" tabindex="-1"></a><span class="co"># Convert DFMs to matrices for caret</span></span>
<span id="cb214-25"><a href="support-vector-machines-svm.html#cb214-25" tabindex="-1"></a></span>
<span id="cb214-26"><a href="support-vector-machines-svm.html#cb214-26" tabindex="-1"></a>matrix_train <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dfm_train)</span>
<span id="cb214-27"><a href="support-vector-machines-svm.html#cb214-27" tabindex="-1"></a>matrix_test <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(dfm_test_matched)</span>
<span id="cb214-28"><a href="support-vector-machines-svm.html#cb214-28" tabindex="-1"></a></span>
<span id="cb214-29"><a href="support-vector-machines-svm.html#cb214-29" tabindex="-1"></a><span class="co"># Remove any zero-variance columns from the training matrix, as caret::train</span></span>
<span id="cb214-30"><a href="support-vector-machines-svm.html#cb214-30" tabindex="-1"></a><span class="co"># can have issues Ensure the test matrix also has these columns removed</span></span>
<span id="cb214-31"><a href="support-vector-machines-svm.html#cb214-31" tabindex="-1"></a></span>
<span id="cb214-32"><a href="support-vector-machines-svm.html#cb214-32" tabindex="-1"></a>nzv_train <span class="ot">&lt;-</span> <span class="fu">nearZeroVar</span>(matrix_train)</span>
<span id="cb214-33"><a href="support-vector-machines-svm.html#cb214-33" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">length</span>(nzv_train) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb214-34"><a href="support-vector-machines-svm.html#cb214-34" tabindex="-1"></a>    matrix_train <span class="ot">&lt;-</span> matrix_train[, <span class="sc">-</span>nzv_train]</span>
<span id="cb214-35"><a href="support-vector-machines-svm.html#cb214-35" tabindex="-1"></a>    matrix_test <span class="ot">&lt;-</span> matrix_test[, <span class="sc">-</span>nzv_train]</span>
<span id="cb214-36"><a href="support-vector-machines-svm.html#cb214-36" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we train the SVM model using <code>caret::train()</code>. <code>caret::train()</code> provides a consistent interface for training various models. We specify <code>method = "svmLinear"</code> for a linear kernel SVM, which is often a good starting point for text data.</p>
<p>Rather than using a single train/test split, we can use k-fold cross-validation to obtain a more reliable estimate of the model’s performance. The <code>caret::train()</code> function makes this easy using the <code>trControl</code> argument. To demonstrate this, we will define a 10-fold cross-validation set-up. For binary classification, the <code>twoClassSummary</code> function calculates metrics such as accuracy, kappa, sensitivity, specificity, and ROC AUC.</p>
<p>What is k-fold cross-validation? Put simply, it involves splitting the training data into equal-sized parts, known as ‘folds’. For instance, if k equals 10, the training data would be divided into ten folds, and the model would be trained ten times. During each iteration, one fold is held out as a validation set and the model is trained using the remaining 9 folds. The trained model is then tested on the held-out validation set, and the performance metrics are recorded. After ten iterations, the performance metrics from each fold are averaged to provide a more reliable estimate of how the model will perform on new data.</p>
<p>The ‘caret’ package makes cross-validation easy. We use the <code>trainControl()</code> function to specify the cross-validation settings and the <code>train()</code> function to train the model. We select a linear SVM using the argument <code>method = "svmLinear"</code>. Linear SVMs are often very effective and computationally less intensive than non-linear ones for text. Setting <code>metric = "ROC"</code> instructs caret to optimise the model based on the area under the ROC curve (AUC), a typical and effective metric for binary classification. Setting <code>classProbs = TRUE</code> means that we require the model to output class probabilities for ROC analysis and calibration plots, such as the probability that a review is ‘positive’. <code>summaryFunction = twoClassSummary</code> is used for binary classification problems and calculates useful metrics such as sensitivity, specificity, and ROC AUC during cross-validation.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="support-vector-machines-svm.html#cb215-1" tabindex="-1"></a>train_control_cv_clf <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb215-2"><a href="support-vector-machines-svm.html#cb215-2" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,          <span class="co"># Use cross-validation</span></span>
<span id="cb215-3"><a href="support-vector-machines-svm.html#cb215-3" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">10</span>,            <span class="co"># Number of folds</span></span>
<span id="cb215-4"><a href="support-vector-machines-svm.html#cb215-4" tabindex="-1"></a>  <span class="at">savePredictions =</span> <span class="st">&quot;final&quot;</span>, <span class="co"># Save predictions for the final model</span></span>
<span id="cb215-5"><a href="support-vector-machines-svm.html#cb215-5" tabindex="-1"></a>  <span class="at">classProbs =</span> <span class="cn">TRUE</span>,    <span class="co"># Compute class probabilities (needed for ROC/Calibration)</span></span>
<span id="cb215-6"><a href="support-vector-machines-svm.html#cb215-6" tabindex="-1"></a>  <span class="at">summaryFunction =</span> twoClassSummary <span class="co"># Use metrics suitable for binary classification</span></span>
<span id="cb215-7"><a href="support-vector-machines-svm.html#cb215-7" tabindex="-1"></a>)</span>
<span id="cb215-8"><a href="support-vector-machines-svm.html#cb215-8" tabindex="-1"></a></span>
<span id="cb215-9"><a href="support-vector-machines-svm.html#cb215-9" tabindex="-1"></a><span class="co"># Train the SVM model using caret::train with cross-validation</span></span>
<span id="cb215-10"><a href="support-vector-machines-svm.html#cb215-10" tabindex="-1"></a><span class="co"># We use svmLinear as the method</span></span>
<span id="cb215-11"><a href="support-vector-machines-svm.html#cb215-11" tabindex="-1"></a><span class="co"># metric = &quot;ROC&quot; tells caret to optimise based on AUC</span></span>
<span id="cb215-12"><a href="support-vector-machines-svm.html#cb215-12" tabindex="-1"></a><span class="co"># Ensure the factor levels for y (polarity_train) are valid R names (&quot;neg&quot;, &quot;pos&quot;)</span></span>
<span id="cb215-13"><a href="support-vector-machines-svm.html#cb215-13" tabindex="-1"></a></span>
<span id="cb215-14"><a href="support-vector-machines-svm.html#cb215-14" tabindex="-1"></a>svm_model_caret_cv <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb215-15"><a href="support-vector-machines-svm.html#cb215-15" tabindex="-1"></a>  <span class="at">x =</span> matrix_train,      <span class="co"># Training feature matrix</span></span>
<span id="cb215-16"><a href="support-vector-machines-svm.html#cb215-16" tabindex="-1"></a>  <span class="at">y =</span> polarity_train,     <span class="co"># Training response vector (factor: &quot;neg&quot;, &quot;pos&quot;)</span></span>
<span id="cb215-17"><a href="support-vector-machines-svm.html#cb215-17" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;svmLinear&quot;</span>, <span class="co"># Use linear kernel SVM</span></span>
<span id="cb215-18"><a href="support-vector-machines-svm.html#cb215-18" tabindex="-1"></a>  <span class="at">trControl =</span> train_control_cv_clf, <span class="co"># Apply cross-validation control for classification</span></span>
<span id="cb215-19"><a href="support-vector-machines-svm.html#cb215-19" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>         <span class="co"># Optimize based on ROC AUC</span></span>
<span id="cb215-20"><a href="support-vector-machines-svm.html#cb215-20" tabindex="-1"></a>)</span>
<span id="cb215-21"><a href="support-vector-machines-svm.html#cb215-21" tabindex="-1"></a></span>
<span id="cb215-22"><a href="support-vector-machines-svm.html#cb215-22" tabindex="-1"></a><span class="fu">print</span>(svm_model_caret_cv)</span></code></pre></div>
<pre><code>## Support Vector Machines with Linear Kernel 
## 
## 1399 samples
##  245 predictor
##    2 classes: &#39;neg&#39;, &#39;pos&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1260, 1258, 1258, 1260, 1260, 1259, ... 
## Resampling results:
## 
##   ROC        Sens       Spec     
##   0.7917106  0.7347418  0.7174979
## 
## Tuning parameter &#39;C&#39; was held constant at a value of 1</code></pre>
<p>The output from <code>print(svm_model_caret_cv)</code> now includes the average classification performance metrics, like Accuracy, Kappa, and ROC AUC, across the 10 folds. This provides a more accurate prediction of how the model will perform on unseen data than a single split would. We then use the trained <code>svm_model_caret_cv</code> to predict the polarity labels and probabilities for the documents in the test set.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="support-vector-machines-svm.html#cb217-1" tabindex="-1"></a><span class="co"># Predict the labels for the test set</span></span>
<span id="cb217-2"><a href="support-vector-machines-svm.html#cb217-2" tabindex="-1"></a>svm_predict_labels_caret_cv <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_model_caret_cv, <span class="at">newdata =</span> matrix_test)</span>
<span id="cb217-3"><a href="support-vector-machines-svm.html#cb217-3" tabindex="-1"></a></span>
<span id="cb217-4"><a href="support-vector-machines-svm.html#cb217-4" tabindex="-1"></a><span class="co"># Predict probabilities for the test set (needed for ROC and Calibration)</span></span>
<span id="cb217-5"><a href="support-vector-machines-svm.html#cb217-5" tabindex="-1"></a>svm_predict_probs_caret_cv <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_model_caret_cv, <span class="at">newdata =</span> matrix_test,</span>
<span id="cb217-6"><a href="support-vector-machines-svm.html#cb217-6" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb217-7"><a href="support-vector-machines-svm.html#cb217-7" tabindex="-1"></a></span>
<span id="cb217-8"><a href="support-vector-machines-svm.html#cb217-8" tabindex="-1"></a><span class="co"># Display the head of the predicted labels and probabilities</span></span>
<span id="cb217-9"><a href="support-vector-machines-svm.html#cb217-9" tabindex="-1"></a><span class="fu">head</span>(svm_predict_labels_caret_cv)</span></code></pre></div>
<pre><code>## [1] pos neg pos neg neg pos
## Levels: neg pos</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="support-vector-machines-svm.html#cb219-1" tabindex="-1"></a><span class="fu">head</span>(svm_predict_probs_caret_cv)</span></code></pre></div>
<pre><code>##         neg       pos
## 1 0.4393709 0.5606291
## 2 0.5076856 0.4923144
## 3 0.4927485 0.5072515
## 4 0.5997361 0.4002639
## 5 0.6494433 0.3505567
## 6 0.4764429 0.5235571</code></pre>
<p>Furthermore, we can examine the classification performance using <code>caret::confusionMatrix()</code> to compare the predicted and actual labels for the test set. This Confusion Matrix is a fundamental tool for evaluating classification models. It is a table that summarises performance by showing:</p>
<ul>
<li><strong>True Positives (TP)</strong>: instances that were correctly predicted as positive</li>
<li><strong>True Negatives (TN)</strong>: instances that were correctly predicted as negative</li>
<li><strong>False Positives (FP)</strong>: instances that were incorrectly predicted as positive</li>
<li><strong>False Negatives (FN)</strong>: instances that were incorrectly predicted as negative</li>
</ul>
<p>From these counts, various metrics are derived:</p>
<ul>
<li><strong>Accuracy: (TP + TN) / Total</strong>, representing overall correctness. However, it can be misleading if the classes are imbalanced;</li>
<li><strong>Sensitivity: TP/(TP+FN)</strong>, indicating how well the model identifies actual positives;</li>
<li><strong>Specificity: TN/(TN+FP)</strong>, showing how well the model identifies actual negatives;</li>
<li><strong>Precision: TP/(TP+FP)</strong>, showing the proportion of predicted positives that were actually positive;</li>
<li><strong>F1-score: 2 * (Precision * Recall) / (Precision + Recall)</strong>, the harmonic mean of precision and recall, is useful when both are important.</li>
</ul>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="support-vector-machines-svm.html#cb221-1" tabindex="-1"></a><span class="co"># Ensure the reference (polarity_test) has the same valid levels as the predicted data</span></span>
<span id="cb221-2"><a href="support-vector-machines-svm.html#cb221-2" tabindex="-1"></a></span>
<span id="cb221-3"><a href="support-vector-machines-svm.html#cb221-3" tabindex="-1"></a>confusion_matrix_caret_cv <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(</span>
<span id="cb221-4"><a href="support-vector-machines-svm.html#cb221-4" tabindex="-1"></a>  <span class="at">data =</span> svm_predict_labels_caret_cv, <span class="co"># Predicted labels (factor)</span></span>
<span id="cb221-5"><a href="support-vector-machines-svm.html#cb221-5" tabindex="-1"></a>  <span class="at">reference =</span> polarity_test            <span class="co"># Actual labels (reference factor)</span></span>
<span id="cb221-6"><a href="support-vector-machines-svm.html#cb221-6" tabindex="-1"></a>)</span>
<span id="cb221-7"><a href="support-vector-machines-svm.html#cb221-7" tabindex="-1"></a></span>
<span id="cb221-8"><a href="support-vector-machines-svm.html#cb221-8" tabindex="-1"></a><span class="co"># Print the confusion matrix and performance statistics</span></span>
<span id="cb221-9"><a href="support-vector-machines-svm.html#cb221-9" tabindex="-1"></a><span class="fu">print</span>(confusion_matrix_caret_cv)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction neg pos
##        neg 219  70
##        pos  88 224
##                                        
##                Accuracy : 0.7371       
##                  95% CI : (0.7, 0.7719)
##     No Information Rate : 0.5108       
##     P-Value [Acc &gt; NIR] : &lt;2e-16       
##                                        
##                   Kappa : 0.4746       
##                                        
##  Mcnemar&#39;s Test P-Value : 0.1762       
##                                        
##             Sensitivity : 0.7134       
##             Specificity : 0.7619       
##          Pos Pred Value : 0.7578       
##          Neg Pred Value : 0.7179       
##              Prevalence : 0.5108       
##          Detection Rate : 0.3644       
##    Detection Prevalence : 0.4809       
##       Balanced Accuracy : 0.7376       
##                                        
##        &#39;Positive&#39; Class : neg          
## </code></pre>
<p>The confusion matrix for the test set — not used during the cross-validation training process — provides a final evaluation of how well the model performs on data that has never been seen before. It shows the number of true positives, false positives and false negatives, from which various metrics can be derived. Finally, we can visualize all this:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="support-vector-machines-svm.html#cb223-1" tabindex="-1"></a>cm_table <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(confusion_matrix_caret_cv<span class="sc">$</span>table)</span>
<span id="cb223-2"><a href="support-vector-machines-svm.html#cb223-2" tabindex="-1"></a></span>
<span id="cb223-3"><a href="support-vector-machines-svm.html#cb223-3" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> cm_table,</span>
<span id="cb223-4"><a href="support-vector-machines-svm.html#cb223-4" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x =</span> Reference, <span class="at">y =</span> Prediction, <span class="at">fill =</span> Freq)) <span class="sc">+</span></span>
<span id="cb223-5"><a href="support-vector-machines-svm.html#cb223-5" tabindex="-1"></a>  <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb223-6"><a href="support-vector-machines-svm.html#cb223-6" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> Freq), <span class="at">vjust =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span> <span class="co"># Add text labels for counts</span></span>
<span id="cb223-7"><a href="support-vector-machines-svm.html#cb223-7" tabindex="-1"></a>  <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">&quot;white&quot;</span>, <span class="at">high =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span> <span class="co"># Colour scale</span></span>
<span id="cb223-8"><a href="support-vector-machines-svm.html#cb223-8" tabindex="-1"></a>  <span class="fu">scale_x_discrete</span>(<span class="at">name =</span> <span class="st">&quot;Actual Polarity&quot;</span>) <span class="sc">+</span></span>
<span id="cb223-9"><a href="support-vector-machines-svm.html#cb223-9" tabindex="-1"></a>  <span class="fu">scale_x_discrete</span>(<span class="at">name =</span> <span class="st">&quot;Predicted Polarity&quot;</span>) <span class="sc">+</span></span>
<span id="cb223-10"><a href="support-vector-machines-svm.html#cb223-10" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Confusion Matrix (SVM Test Set)&quot;</span>) <span class="sc">+</span></span>
<span id="cb223-11"><a href="support-vector-machines-svm.html#cb223-11" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="co"># Use a minimal theme</span></span>
<span id="cb223-12"><a href="support-vector-machines-svm.html#cb223-12" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>)) <span class="co"># Rotate x-axis labels if needed</span></span></code></pre></div>
<pre><code>## Scale for x is already present.
## Adding another scale for x, which will replace the existing scale.</code></pre>
<p><img src="Introduction-to-Quantitative-Text-Analysis_files/figure-html/svm-polarity-confusion-graph-1.png" width="672" /></p>
<p>In classification using linear SVMs, variable importance is usually related to the size of the model coefficients, showing which features contribute most to class separation. The <code>caret::varImp()</code> function can extract this information:</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="support-vector-machines-svm.html#cb225-1" tabindex="-1"></a>var_importance_svm <span class="ot">&lt;-</span> <span class="fu">varImp</span>(svm_model_caret_cv, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb225-2"><a href="support-vector-machines-svm.html#cb225-2" tabindex="-1"></a></span>
<span id="cb225-3"><a href="support-vector-machines-svm.html#cb225-3" tabindex="-1"></a><span class="co"># Print the variable importance</span></span>
<span id="cb225-4"><a href="support-vector-machines-svm.html#cb225-4" tabindex="-1"></a><span class="fu">print</span>(var_importance_svm)</span></code></pre></div>
<pre><code>## ROC curve variable importance
## 
##   only 20 most important variables shown (out of 245)
## 
##           Importance
## bad           0.6239
## great         0.5993
## worst         0.5731
## even          0.5653
## movie         0.5547
## best          0.5508
## love          0.5490
## just          0.5484
## acting        0.5479
## plot          0.5464
## many          0.5449
## waste         0.5424
## also          0.5417
## nothing       0.5413
## minutes       0.5404
## better        0.5399
## boring        0.5399
## like          0.5398
## worse         0.5395
## excellent     0.5391</code></pre>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="support-vector-machines-svm.html#cb227-1" tabindex="-1"></a><span class="co"># Plot the top 20 most important variables</span></span>
<span id="cb227-2"><a href="support-vector-machines-svm.html#cb227-2" tabindex="-1"></a><span class="fu">plot</span>(var_importance_svm, <span class="at">top =</span> <span class="dv">20</span>, <span class="at">main =</span> <span class="st">&quot;Variable Importance (SVM - Top 20 words)&quot;</span>)</span></code></pre></div>
<p><img src="Introduction-to-Quantitative-Text-Analysis_files/figure-html/svm-polarity-variable-importance-1.png" width="672" /></p>
<p>This graph illustrates the words that greatly influenced the SVM model’s polarity classification decisions. Words with high importance may have large positive or negative coefficients, which push documents towards the ‘pos’ or ‘neg’ class boundary.</p>
<p>Next, we plot the Receiver Operating Characteristic (ROC) curve using the predicted probabilities from the test set and the actual test labels and calculate the Area Under the Curve (AUC). The ROC curve illustrates the trade-off between sensitivity and specificity as the classification threshold varies.</p>
<p>The ROC curve is another common way to evaluate binary classifiers. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various classification threshold settings. A model that performs no better than random guessing would have a ROC curve close to the diagonal line (from bottom-left to top-right). A good model will have an ROC curve that bows towards the top-left corner. The area under the curve (AUC) summarises the ROC curve into a single number. An AUC of 1 indicates a perfect classifier; an AUC of 0.5 indicates performance no better than random chance; and an AUC of less than 0.5 suggests performance worse than random chance, indicating something is likely wrong, such as the flipped labels.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="support-vector-machines-svm.html#cb228-1" tabindex="-1"></a><span class="co"># The roc() function needs the actual responses (factor) and the predicted</span></span>
<span id="cb228-2"><a href="support-vector-machines-svm.html#cb228-2" tabindex="-1"></a><span class="co"># probabilities for the positive class (numeric)</span></span>
<span id="cb228-3"><a href="support-vector-machines-svm.html#cb228-3" tabindex="-1"></a>roc_obj_svm <span class="ot">&lt;-</span> <span class="fu">roc</span>(<span class="at">response =</span> polarity_test, <span class="at">predictor =</span> svm_predict_probs_caret_cv[,</span>
<span id="cb228-4"><a href="support-vector-machines-svm.html#cb228-4" tabindex="-1"></a>    <span class="st">&quot;pos&quot;</span>], <span class="at">levels =</span> <span class="fu">levels</span>(polarity_test))</span>
<span id="cb228-5"><a href="support-vector-machines-svm.html#cb228-5" tabindex="-1"></a><span class="fu">plot</span>(roc_obj_svm, <span class="at">main =</span> <span class="st">&quot;ROC Curve (SVM Test Set)&quot;</span>, <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>)</span>
<span id="cb228-6"><a href="support-vector-machines-svm.html#cb228-6" tabindex="-1"></a></span>
<span id="cb228-7"><a href="support-vector-machines-svm.html#cb228-7" tabindex="-1"></a>auc_value_svm <span class="ot">&lt;-</span> <span class="fu">auc</span>(roc_obj_svm)  <span class="co"># Add AUC value to the plot</span></span>
<span id="cb228-8"><a href="support-vector-machines-svm.html#cb228-8" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="at">legend =</span> <span class="fu">paste</span>(<span class="st">&quot;AUC =&quot;</span>, <span class="fu">round</span>(auc_value_svm, <span class="dv">3</span>)), <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>)</span></code></pre></div>
<p><img src="Introduction-to-Quantitative-Text-Analysis_files/figure-html/svm-polarity-roc-1.png" width="672" /></p>
<p>The ROC curve and the area under the curve (AUC) provide insight into the model’s ability to distinguish between positive and negative classes. An AUC close to 1 indicates excellent discrimination, whereas an AUC close to 0.5 suggests that the model performs no better than by chance.</p>
<p>Finally, we can examine the calibration plot. This shows how well the predicted probabilities match the observed frequencies in the test set. In a well-calibrated model, the points should lie close to the diagonal line. This indicates that a predicted probability of 0.8, for example, corresponds to the positive class occurring in approximately 80% of instances with that predicted probability. A calibration plot helps us to assess whether the model’s predicted probabilities are reliable. For example, suppose the model predicts a probability of 0.8 for a set of reviews being ‘positive’. In that case, we hope about 80% of those reviews are positive—the plot groups predictions by probability score. For example, it groups all reviews where P(positive) is between 0.7 and 0.8 and plots the proportion of positive reviews observed in each group against the predicted probability. A perfectly calibrated model would have points along the diagonal line (y = x).</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="support-vector-machines-svm.html#cb229-1" tabindex="-1"></a><span class="co"># Create a data frame containing the observed outcomes and the predicted probabilities</span></span>
<span id="cb229-2"><a href="support-vector-machines-svm.html#cb229-2" tabindex="-1"></a></span>
<span id="cb229-3"><a href="support-vector-machines-svm.html#cb229-3" tabindex="-1"></a>calibration_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">obs =</span> polarity_test, <span class="at">prob_pos =</span> svm_predict_probs_caret_cv[, <span class="st">&quot;pos&quot;</span>])</span>
<span id="cb229-4"><a href="support-vector-machines-svm.html#cb229-4" tabindex="-1"></a></span>
<span id="cb229-5"><a href="support-vector-machines-svm.html#cb229-5" tabindex="-1"></a><span class="co"># Compute calibration information</span></span>
<span id="cb229-6"><a href="support-vector-machines-svm.html#cb229-6" tabindex="-1"></a></span>
<span id="cb229-7"><a href="support-vector-machines-svm.html#cb229-7" tabindex="-1"></a>calibration_obj_svm <span class="ot">&lt;-</span> <span class="fu">calibration</span>(</span>
<span id="cb229-8"><a href="support-vector-machines-svm.html#cb229-8" tabindex="-1"></a>  obs <span class="sc">~</span> prob_pos,</span>
<span id="cb229-9"><a href="support-vector-machines-svm.html#cb229-9" tabindex="-1"></a>  <span class="co"># Formula: observed variable ~ predicted probability variable</span></span>
<span id="cb229-10"><a href="support-vector-machines-svm.html#cb229-10" tabindex="-1"></a>  <span class="at">data =</span> calibration_data,</span>
<span id="cb229-11"><a href="support-vector-machines-svm.html#cb229-11" tabindex="-1"></a>  <span class="co"># The data frame containing these variables</span></span>
<span id="cb229-12"><a href="support-vector-machines-svm.html#cb229-12" tabindex="-1"></a>  <span class="at">class =</span> <span class="st">&quot;pos&quot;</span>,</span>
<span id="cb229-13"><a href="support-vector-machines-svm.html#cb229-13" tabindex="-1"></a>  <span class="co"># Specify the positive class name from the &#39;obs&#39; column</span></span>
<span id="cb229-14"><a href="support-vector-machines-svm.html#cb229-14" tabindex="-1"></a>  <span class="at">cuts =</span> <span class="dv">10</span>,</span>
<span id="cb229-15"><a href="support-vector-machines-svm.html#cb229-15" tabindex="-1"></a>  <span class="co"># Number of bins (quantiles) for grouping probabilities</span></span>
<span id="cb229-16"><a href="support-vector-machines-svm.html#cb229-16" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;quantile&quot;</span> <span class="co"># Method for creating bins</span></span>
<span id="cb229-17"><a href="support-vector-machines-svm.html#cb229-17" tabindex="-1"></a>)</span>
<span id="cb229-18"><a href="support-vector-machines-svm.html#cb229-18" tabindex="-1"></a></span>
<span id="cb229-19"><a href="support-vector-machines-svm.html#cb229-19" tabindex="-1"></a>calibration_data_for_plot <span class="ot">&lt;-</span> calibration_obj_svm<span class="sc">$</span>data</span>
<span id="cb229-20"><a href="support-vector-machines-svm.html#cb229-20" tabindex="-1"></a></span>
<span id="cb229-21"><a href="support-vector-machines-svm.html#cb229-21" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> calibration_data_for_plot, <span class="fu">aes</span>(<span class="at">x =</span> midpoint, <span class="at">y =</span> Percent)) <span class="sc">+</span></span>
<span id="cb229-22"><a href="support-vector-machines-svm.html#cb229-22" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> <span class="co"># Plot the calibration line connecting the points</span></span>
<span id="cb229-23"><a href="support-vector-machines-svm.html#cb229-23" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="co"># Plot the points for each bin</span></span>
<span id="cb229-24"><a href="support-vector-machines-svm.html#cb229-24" tabindex="-1"></a>  <span class="fu">geom_abline</span>(</span>
<span id="cb229-25"><a href="support-vector-machines-svm.html#cb229-25" tabindex="-1"></a>    <span class="at">intercept =</span> <span class="dv">0</span>,</span>
<span id="cb229-26"><a href="support-vector-machines-svm.html#cb229-26" tabindex="-1"></a>    <span class="at">slope =</span> <span class="dv">1</span>,</span>
<span id="cb229-27"><a href="support-vector-machines-svm.html#cb229-27" tabindex="-1"></a>    <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>,</span>
<span id="cb229-28"><a href="support-vector-machines-svm.html#cb229-28" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;darkred&quot;</span></span>
<span id="cb229-29"><a href="support-vector-machines-svm.html#cb229-29" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb229-30"><a href="support-vector-machines-svm.html#cb229-30" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">name =</span> <span class="st">&quot;Bin Midpoint&quot;</span>, <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>)) <span class="sc">+</span></span>
<span id="cb229-31"><a href="support-vector-machines-svm.html#cb229-31" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">name =</span> <span class="st">&quot;Observed Event Percentage&quot;</span>, <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>)) <span class="sc">+</span></span>
<span id="cb229-32"><a href="support-vector-machines-svm.html#cb229-32" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Calibration Plot&quot;</span>) <span class="sc">+</span></span>
<span id="cb229-33"><a href="support-vector-machines-svm.html#cb229-33" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb229-34"><a href="support-vector-machines-svm.html#cb229-34" tabindex="-1"></a>  <span class="fu">coord_equal</span>()</span></code></pre></div>
<p><img src="Introduction-to-Quantitative-Text-Analysis_files/figure-html/svm-polarity-calibration-1.png" width="672" /></p>
<p>The closer the black line representing the model’s calibration is to the red dashed line representing perfect calibration, the more reliable the model’s probabilities are. Deviations from this indicate either overconfidence, where the line is above the diagonal for low probabilities and below it for high probabilities or underconfidence. Poor calibration means that, while the model may make the correct classification, the stated confidence level (i.e. the probability) may be unreliable.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="supervised-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["Introduction to Quantitative Text Analysis.pdf", "Introduction to Quantitative Text Analysis.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  },
  "toolbar": {
    "position": "static"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
